# Integrity of NLP models

## Alignment of NLP models

### Examples of work 
- [ALIGNING AI WITH SHARED HUMAN VALUES](https://arxiv.org/pdf/2008.02275.pdf)
- [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)
- [truthfulQA](https://github.com/sylinrl/TruthfulQA)
- [AllenNLP Interpret](https://arxiv.org/abs/1909.09251)


## adversarial attack (mt, adversaraial, black; another gitproject)

### Examples of work
- [adversarial attack on MT models](https://arxiv.org/abs/2004.15015)
- [Universarial adversarial triggers](https://arxiv.org/abs/1908.07125)
- [Adversarial Semantic Collisions](https://arxiv.org/pdf/2011.04743.pdf)
- project [TextAttack](https://github.com/QData/TextAttack) 
### [My exploration](https://github.com/ruiyeNLP/adversarial_attack) 
### backdoor attack 
- [Concealed Data Poisoning Attacks on NLP Models](https://arxiv.org/abs/2010.12563)


# Confidentiality of NLP models

### Examples of work 
- [membership inference on text-generation models](https://arxiv.org/pdf/1811.00513.pdf)
- [training data extraction](https://arxiv.org/abs/2012.07805)
- [Information Leakage in Embedding Models](https://arxiv.org/pdf/2004.00053.pdf)
- model stealing attack 



